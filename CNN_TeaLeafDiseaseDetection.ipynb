{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1AU4SvWh9PwWRS66lwCJy71jO685L5wSm",
      "authorship_tag": "ABX9TyPDPlEpOJPo3ukunjFmQNsC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhawaDong/Tea-Leaf-Disease-Detection/blob/main/CNN_TeaLeafDiseaseDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import albumentations\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader #TensorDataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "metadata": {
        "id": "GP0DWP5ZISdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' SEED Everything '''\n",
        "def seed_everything(SEED=42):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "SEED=42\n",
        "seed_everything(SEED=SEED)\n",
        "''' SEED Everything '''\n",
        "# set computation device\n",
        "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\")"
      ],
      "metadata": {
        "id": "mGTOEAFXK86f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to create csv file of image path and corresponding image class\n",
        "def create_image_csv(root_folder, csv_filename):\n",
        "    with open(csv_filename, 'w', newline='') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow(['Class', 'Image_Path'])  # Header\n",
        "\n",
        "        for class_name in os.listdir(root_folder):\n",
        "            class_folder = os.path.join(root_folder, class_name)\n",
        "\n",
        "            if os.path.isdir(class_folder):\n",
        "                for image_name in os.listdir(class_folder):\n",
        "                    if image_name.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "                        image_path = os.path.join(class_folder, image_name)\n",
        "                        csv_writer.writerow([class_name, image_path])"
      ],
      "metadata": {
        "id": "53gtAtJXHkng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder_path = '/content/drive/MyDrive/7#MachineLearning#Projects/tea sickness dataset'\n",
        "csv_file_path = 'TeaLeaf_ImageDataSet.csv'\n",
        "\n",
        "# Call the function to create the CSV file\n",
        "create_image_csv(root_folder_path, csv_file_path)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Display the DataFrame\n",
        "#print(df)\n",
        "\n",
        "#save the csv file to google drive ..\n",
        "#df.to_csv('/content/drive/MyDrive/7#MachineLearning#Projects/TeaLeaf_ImageDataSet.csv') # + csv_file_path)"
      ],
      "metadata": {
        "id": "kF8gz6usc7gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LeafDataSet = pd.read_csv('/content/drive/MyDrive/7#MachineLearning#Projects/TeaLeaf_ImageDataSet.csv');\n",
        "#Imageinput ant classes....\n",
        "LeafDataSet[\"Class\"].replace({\"white spot\": 0, \"Anthracnose\": 1,\n",
        "                              \"healthy\": 2,   \"brown blight\": 3,\n",
        "                              \"bird eye spot\": 4, \"algal leaf\": 5,\n",
        "                              \"red leaf spot\": 6, \"gray light\": 7}, inplace = True)"
      ],
      "metadata": {
        "id": "LdHQ01_pRVux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#randomaize the dataset\n",
        "Rand_LeafDataSet = LeafDataSet.sample(frac=1, random_state=46)"
      ],
      "metadata": {
        "id": "sBZ3iivWUIno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = Rand_LeafDataSet.Image_Path.values   #Class', 'Image_Path'\n",
        "y = Rand_LeafDataSet.Class.values"
      ],
      "metadata": {
        "id": "e9yp0etNcXbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(xtrain, xtest, ytrain, ytest) = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "id": "sbW8CpDY2-Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ytrain.size)"
      ],
      "metadata": {
        "id": "uJ9PqOalQ284"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#frequency of class in TrainSet\n",
        "(unique, counts) = np.unique(ytrain, return_counts=True)\n",
        "Trainfrequencies = np.asarray((unique, counts)).T\n",
        "#Trainfrequencies"
      ],
      "metadata": {
        "id": "Yslui4Bl3ec0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image dataset module\n",
        "class DDSMimageDataset(Dataset):\n",
        "    def __init__(self, path, labels, tfms=None):\n",
        "        self.X = path\n",
        "        self.y = labels\n",
        "        # apply augmentations\n",
        "        if tfms == 0:  # if validating\n",
        "            self.aug = albumentations.Compose([\n",
        "                albumentations.Resize(224, 224, always_apply=True),\n",
        "                albumentations.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], always_apply=True)\n",
        "            ])\n",
        "        else:  # if training\n",
        "            self.aug = albumentations.Compose([\n",
        "                albumentations.Resize(224, 224, always_apply=True),\n",
        "                albumentations.HorizontalFlip(p=0.5),  # Adjust probability as needed\n",
        "                albumentations.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.3, rotate_limit=30, p=0.5),  # Adjust parameters as needed\n",
        "                albumentations.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], always_apply=True)\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image = Image.open(self.X[i])\n",
        "        image = self.aug(image=np.array(image))['image']\n",
        "        label = self.y[i]\n",
        "        # Transpose the image to have channels first (C, H, W)\n",
        "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "        return torch.tensor(image, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "pS8hDXwd4WF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BatchSize = 64\n",
        "#trinaing data_set\n",
        "train_data = DDSMimageDataset(xtrain, ytrain, tfms=1)\n",
        "train_loader = DataLoader(train_data, batch_size=BatchSize, shuffle=True)\n",
        "\n",
        "#testing data_set\n",
        "test_data = DDSMimageDataset(xtest, ytest, tfms=0)\n",
        "test_loader = DataLoader(test_data, batch_size=BatchSize, shuffle=False)"
      ],
      "metadata": {
        "id": "3mqEfrpK47LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN architecture\n",
        "class TeaLeafClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=8):  # Specify the number of classes\n",
        "        super(TeaLeafClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        #self.fc1 = nn.Linear(32 * 16 * 16, 256)\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 256)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, num_classes)  # Adjust the number of output neurons\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        #print(\"After conv1:\", x.size())\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        #print(\"After conv2:\", x.size())\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        #x = x.view(-1, 32 * 16 * 16)\n",
        "        x = x.view(-1, 32 * 56 * 56)  # Reshape for fully connected layer\n",
        "        #print(\"After flattening:\", x.size())\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_3NXCjgSIhAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = TeaLeafClassifier(num_classes=8).to(device)"
      ],
      "metadata": {
        "id": "RnJco5_dInyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "tnYloJEuIsZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images.to(device))\n",
        "        #loss = criterion(outputs, torch.argmax(labels.to(device), dim=1))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "x7V6V_dr8cHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images.to(device))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "H_pd0KLWIx4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wgfMPZpdvmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}